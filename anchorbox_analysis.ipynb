{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backbone import EfficientDetBackbone\n",
    "from efficientdet.model import Regressor, Classifier\n",
    "from efficientdet.loss import FocalLoss, calc_iou\n",
    "from efficientdet.utils import Anchors, BBoxTransform\n",
    "import torch\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientDetBackbone(num_classes=2, compound_coef=0,\n",
    "                             ratios=[(1.4, 0.7), (1.0, 1.0), (0.7, 1.4), (0.5, 1.5), (0.4, 1.6)], \n",
    "                             scales=[2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])\n",
    "\n",
    "# model = EfficientDetBackbone(num_classes=2, compound_coef=0,\n",
    "#                              ratios=[(1.0, 1.0), (1.4, 0.7), (0.7, 1.4)], \n",
    "#                              scales=[2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 81840, 4]),\n",
       " torch.Size([1, 81840, 2]),\n",
       " torch.Size([1, 81840, 4]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = np.random.randn(1,3,512,512)\n",
    "img = torch.from_numpy(img).float()\n",
    "a,b,c,d = model(img)\n",
    "b.shape, c.shape, d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = Regressor(in_channels=self.fpn_num_filters[self.compound_coef], num_anchors=num_anchors,\n",
    "                           num_layers=self.box_class_repeats[self.compound_coef],\n",
    "                           pyramid_levels=self.pyramid_levels[self.compound_coef])\n",
    "classifier = Classifier(in_channels=self.fpn_num_filters[self.compound_coef], num_anchors=num_anchors,\n",
    "                             num_classes=num_classes,\n",
    "                             num_layers=self.box_class_repeats[self.compound_coef],\n",
    "                             pyramid_levels=self.pyramid_levels[self.compound_coef])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.compound_coef = 0\n",
    "\n",
    "self.backbone_compound_coef = [0, 1, 2, 3, 4, 5, 6, 6, 7]\n",
    "self.fpn_num_filters = [64, 88, 112, 160, 224, 288, 384, 384, 384]\n",
    "self.fpn_cell_repeats = [3, 4, 5, 6, 7, 7, 8, 8, 8]\n",
    "self.input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536, 1536]\n",
    "self.box_class_repeats = [3, 3, 3, 4, 4, 4, 5, 5, 5]\n",
    "self.pyramid_levels = [5, 5, 5, 5, 5, 5, 5, 5, 6]\n",
    "self.anchor_scale = [4., 4., 4., 4., 4., 4., 4., 5., 4.]\n",
    "self.aspect_ratios = kwargs.get('ratios', [(1.0, 1.0), (1.4, 0.7), (0.7, 1.4)])\n",
    "self.num_scales = len(kwargs.get('scales', [2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)]))\n",
    "conv_channel_coef = {\n",
    "    # the channels of P3/P4/P5.\n",
    "    0: [40, 112, 320],\n",
    "    1: [40, 112, 320],\n",
    "    2: [48, 120, 352],\n",
    "    3: [48, 136, 384],\n",
    "    4: [56, 160, 448],\n",
    "    5: [64, 176, 512],\n",
    "    6: [72, 200, 576],\n",
    "    7: [72, 200, 576],\n",
    "    8: [80, 224, 640],\n",
    "}\n",
    "\n",
    "num_anchors = len(self.aspect_ratios) * self.num_scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyramid_levels = [2, 3, 4, 5]\n",
    "strides = [2**x for x in pyramid_levels]\n",
    "scales = np.array([2 ** 0, 2 ** (1.0 / 3.0), 2 ** (2.0 / 3.0)])\n",
    "ratios = [(1.4, 0.7), (1.0, 1.0), (0.7, 1.4), (0.5, 1.5), (0.4, 1.6)]\n",
    "anchor_scale = 4.\n",
    "image_shape = (512,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_all = []\n",
    "for stride in strides:\n",
    "    boxes_level = []\n",
    "    for scale, ratio in itertools.product(scales, ratios):\n",
    "        if image_shape[1] % stride != 0:\n",
    "            raise ValueError('input size must be divided by the stride.')\n",
    "        base_anchor_size = anchor_scale * stride * scale\n",
    "        anchor_size_x_2 = base_anchor_size * ratio[0] / 2.0\n",
    "        anchor_size_y_2 = base_anchor_size * ratio[1] / 2.0\n",
    "\n",
    "        x = np.arange(stride / 2, image_shape[1], stride)\n",
    "        y = np.arange(stride / 2, image_shape[0], stride)\n",
    "        xv, yv = np.meshgrid(x, y)\n",
    "        xv = xv.reshape(-1)\n",
    "        yv = yv.reshape(-1)\n",
    "\n",
    "        # y1,x1,y2,x2\n",
    "        boxes = np.vstack((yv - anchor_size_y_2, xv - anchor_size_x_2,\n",
    "                           yv + anchor_size_y_2, xv + anchor_size_x_2))\n",
    "        boxes = np.swapaxes(boxes, 0, 1)\n",
    "        boxes_level.append(np.expand_dims(boxes, axis=1))\n",
    "    # concat anchors on the same level to the reshape NxAx4\n",
    "    boxes_level = np.concatenate(boxes_level, axis=1)\n",
    "    boxes_all.append(boxes_level.reshape([-1, 4]))\n",
    "\n",
    "anchor_boxes = np.vstack(boxes_all)\n",
    "\n",
    "anchor_boxes = torch.from_numpy(anchor_boxes)\n",
    "anchor_boxes = anchor_boxes.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 326400, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_boxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_all = []\n",
    "image_shape = (512,512)\n",
    "for stride in strides:\n",
    "    boxes_level = []\n",
    "    \n",
    "    scale, ratio = (scales[0], ratios[3])\n",
    "    if image_shape[1] % stride != 0:\n",
    "        raise ValueError('input size must be divided by the stride.')\n",
    "    base_anchor_size = anchor_scale * stride * scale\n",
    "    anchor_size_x_2 = base_anchor_size * ratio[0] / 2.0\n",
    "    anchor_size_y_2 = base_anchor_size * ratio[1] / 2.0\n",
    "\n",
    "    x = np.arange(stride / 2, image_shape[1], stride)\n",
    "    y = np.arange(stride / 2, image_shape[0], stride)\n",
    "    xv, yv = np.meshgrid(x, y)\n",
    "    xv = xv.reshape(-1)\n",
    "    yv = yv.reshape(-1)\n",
    "\n",
    "    # y1,x1,y2,x2\n",
    "    boxes = np.vstack((yv - anchor_size_y_2, xv - anchor_size_x_2,\n",
    "                       yv + anchor_size_y_2, xv + anchor_size_x_2))\n",
    "    boxes = np.swapaxes(boxes, 0, 1)\n",
    "    boxes_level.append(np.expand_dims(boxes, axis=1))\n",
    "    # concat anchors on the same level to the reshape NxAx4\n",
    "    boxes_level = np.concatenate(boxes_level, axis=1)\n",
    "    boxes_all.append(boxes_level.reshape([-1, 4]))\n",
    "\n",
    "anchor_boxes = np.vstack(boxes_all)\n",
    "\n",
    "anchor_boxes = torch.from_numpy(anchor_boxes)\n",
    "anchor_boxes = anchor_boxes.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 87296, 4])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_boxes.long().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
